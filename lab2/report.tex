\documentclass{report}

\usepackage{siunitx}
\sisetup{
  round-mode = places,
  round-precision = 3
}
% Format a query and the question the results should answer
\newcommand{\query}[2]{\item\textit{#1}\\#2}

% Format an information need
\newcommand{\infoneed}[2]{
\item \begin{list}{}{}
  \item #1

  \item \itshape#2
  \end{list}
}

% Import a query results file
\newcommand{\results}[2]{\subsection{Query #1 Results}\begin{list}{}{}\input{data/#2}\end{list}}
% Format each result record
\newenvironment{result}[2]{\item #2 \hfill{\small (\num{#1})}\begin{quote}}{\end{quote}}
% Import an info need results file
\newcommand{\needresults}[2]{\subsection{Info Need #1 Results}\begin{list}{}{}\input{data/#2}\end{list}}

% Allow customizing algorithm names
\newcommand{\okapi}{okapi }
\newcommand{\cosine}{cosine similarity }

\title{CPE466\\Lab2}
\author{
  Gilbert, Andrew\\
  \texttt{apgilber@calpoly.edu}
  \and
  Terrell, Josh\\
  \texttt{jmterrel@calpoly.edu}
}
\date{}

\begin{document}

\maketitle

\begin{abstract}
Given some need for information, information retrieval systems are
used to find a relevant subset of documents in a large collection of
documents. To demonstrate and solidify our understanding, we
implemented a simple IR system to query for utterences from the
\textsc{Vaccination-Discussion} dataset. We implemented two matching
algorithms, \cosine and \okapi, to build a
result set of relevant documents for a given query.
\end{abstract}

\section{Introduction}
We built a basic IR system to retrieve relevant documents for a given
query. The system includes a component for building vector-space
models for word occurences in documents, and a matching component to
digest these models and ultimatley determine the most relevant
documents to a given query.

\section{System Design and Implementation}
The system consists of two primary components: parsing and query
matching. The parsing component prepares a database of document
vectors and document collection meta data. The query matching
component uses the prepared database for finding the most relevant
documents to the given query.

The parsing component reads in and parses words from the provided JSON
document which serves as the raw document collection. The parsing
phase also does stopword removal and stemming on the document text. A
vector of inner-document word occurrences is appended to each document
and the whole collection is saved to a file. Meta-information
about the collection as a whole, for instance the number of documents
and the document word frequency, is also saved to a file.

The query matching component of the system uses the databases built by
the parsing component to more efficiently find relevant documents to a
given query. It returns the top ten relevant documents, by
default. The query matching component also has command-line flags to
switch between the two implemented matching algorithms, \cosine and
\okapi.

\section{Query Answering}
% I think we might want to get rid of the italics on the algorithm names.
We executed the queries provided using both the \cosine and \okapi
matching algorithms. For each query, both algorithms were evaluated on
their ability to produce relevant documents in the top ten results
(excluding Q4, for which we only examined the first result). The
results are displayed in \Vref{document-relevance}. The full query
responses are provided in \vref{query-responses}.

To determine how well the queries produced relevant documents, we
defined an explicit yes/no question per query. The questions we asked
are paired below with each query.

\begin{enumerate}[Q1:]
  \query{Disneyland incident measles people sick}{Is the document
about a Disneyland measles incident?}

  \query{Centers for Disease Control chance of a serious allergic
reaction}{Is the document relevant to both the Centers for Disease
Control and allergic reactions?}

  \query{requiring parents to jump through hoops}{Is the document
about jumping through hoops?}

  \query{Nearly everyone, from my father's generation and generations
older than him, when I'd go and speak to groups, talk about the
stories that they have both from their own childhood, from friends,
personal stories of near-death experiences, friends they had who were
maimed by communicable diseases.}{Does the first result contain the
exact match searched for?}

  \query{parents' right to make healthcare decisions for their
child}{Is the document about parents' rights to make decisions for
their children?}

  \query{disabled kids children exceptional needs right to
education}{Is the document about disabled children or children with
exceptional needs having a right to education?}

  \query{can a licensed physician in the state of California provide
an exemption from immunization?}{Is the document about licensed
physicians providing immunization exemptions?}

  \query{association oppose bill}{Is the document about an association
which opposes the bill?}

  \query{mercury in vaccines}{Is the document about murcury in
vaccines?}

  \query{disease exposure sustained transmission opt-out personal
belief exemption preventable}{Is the document about how sustained transmission after disease exposure would be preventable without the personal belief exemption?}
\end{enumerate}

\begin{table}
  \begin{center}
    \begin{tabu}{c c c}
      \toprule
      Query & Cosine Similarity &  Okapi\\
      \midrule
      1 & 0 & 10\\
      2 & 1 & 2\\
      3 & 2 & 2\\
      4 & y & y\\
      5 & 5 & 10\\
      6 & 2 & 8\\
      7 & 4 & 8\\
      8 & 4 & 2\\
      9 & 6 & 8\\
      10 & 1 & 0\\
      \bottomrule
    \end{tabu}
  \end{center}
  \caption{Relevant documents in top ten results}
  \label{document-relevance}
\end{table}

When running the queries through the system, we observed patterns in
the documents that were irrelevant to the queries. One prevailing
observed pattern is that the \cosine algorithm
tended to produce many large irrelevant documents in the top ten
results. For many of the queries, the same large documents were listed
in the results. On the other hand, the \okapi algorithm
produced half way relevant documents. For instance, the results for Q2
returned many documents about the Centers for Disease Control or
allergic reaction, but had only two documents relevant to both things.

It also seems worth noting that it is difficult to tell what the user
is trying to find with Q10. The question given was a guess at the
intended goal. In reality, the results are often related to parts of
the query but not the stated question for Q10.

\section{Information Need Matching}
We developed a query for each information need and executed each query
using only the \okapi matching algorithm. % is this correct?
The information needs are shown in \vref{needs-and-queries} with their respective queries.
We then counted the number of results for each query which answered
the information need requested. \Vref{need-results} summarizes the
precision of the results.
The full lists of results for each query are available in \vref{need-responses}.

\begin{table}
  \begin{center}
    \begin{tabu}{c c c}
      \toprule
      Query & Okapi\\
      \midrule
      1 & 3\\
      2 & 6\\
      3 & 9\\
      4 & 2\\
      5 & 2\\
      \bottomrule
    \end{tabu}
  \end{center}
  \caption{Relevant documents in top ten results}
  \label{need-results}
\end{table}

\subsection{Information Needs and Associated Queries}
\label{needs-and-queries}
\begin{enumerate}[{I}1:]
\infoneed{I would like to know who among the legislators have been strongly opposed to the vaccination bill, and specifically, I would like to see what their arguments were}{<PersonType:Legislator> freedom choice against oppose}
\infoneed{I would like to find out the chief arguments of the proponents of the legislation. Why did they think it was important to require the vaccination of every child attending school in the state?}{reason sick danger disease}
\infoneed{The need for vaccination, as well as the opposition to it is often gauged in terms of medical necessity and/or medical needs of children. I am interested in finding the instances of such justifications, both for and against the vaccination.}{need against support oppose sick death danger vulnerable children}
\infoneed{The SB 277 bill has gone through a series of modifications and amendments during the timeframe of the dataset (the committee hearings took place on different dates). I am interested in finding out more about any specific changes the bill, amendments that the authors wound up including, or any amendments that might have been discussed regardless of whether or not they made it into the future versions of the bill.}{amend change previous last before bill}
\infoneed{The proponents of the SB277 bill often cite scientific evidence to back their case for the bill. I am interested in finding utternaces in which the speaker expressed doubt in the science supporting vaccination.}{evidence research science doubt not false actually proof claim faulty poor adequate contrary belief oppose against}
\end{enumerate}

\section{Analysis and Conclusions}
We initially implemented the basic \cosine matching, but we discovered
that it gave too much weight to large documents with many words. We
then implemented \okapi and noticed that our results tended to be much
more relevant.

For both algorithms, we hypothesized that word proximity may also help
in computing more relevant results. For instance, in Q3, having
proximity may give more weight to the words in the phrase ``jump
through hoops'' occurring close together and in the correct order.

Ultimately, we learned some details about IR systems that might have
been much more difficult to learn without the hands-on experience. For
instance, we had no idea that iterating over four thousand documents
per query would take more than a second, as it did. Using an inverted
index would help dramatically with this performance issue.

\appendix
\section{Query Results}
\textbf{Bold} results are those we classified as relevant.
\label{query-responses}
\results{1}{queryresults01.txt.tex}
\results{2}{queryresults02.txt.tex}
\results{3}{queryresults03.txt.tex}
\results{4}{queryresults04.txt.tex}
\results{5}{queryresults05.txt.tex}
\results{6}{queryresults06.txt.tex}
\results{7}{queryresults07.txt.tex}
\results{8}{queryresults08.txt.tex}
\results{9}{queryresults09.txt.tex}
\results{10}{queryresults10.txt.tex}
\section{Info Needs Query Results}
\label{need-responses}
\textbf{Bold} results are those we classified as relevant.
\needresults{1}{InfoNeed01.txt.tex}
\needresults{2}{InfoNeed02.txt.tex}
\needresults{3}{InfoNeed03.txt.tex}
\needresults{4}{InfoNeed04.txt.tex}
\needresults{5}{InfoNeed05.txt.tex}
\end{document}
